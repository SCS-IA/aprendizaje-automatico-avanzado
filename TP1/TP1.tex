\documentclass{beamer}

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[spanish]{babel}
\usepackage{graphicx}
\usepackage{ragged2e}
\usepackage[table]{xcolor}
\usepackage{listings}

\usefonttheme[onlymath]{serif}

\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{backcolor}{rgb}{0.95,0.95,0.95}
\definecolor{verdecelda}{HTML}{B7CBA6}
\definecolor{rojocelda}{HTML}{FF9999}
\definecolor{lightgray}{gray}{0.6}


\usetheme{CambridgeUS}
\setbeamertemplate{navigation symbols}{}
\setbeamertemplate{blocks}[rounded][shadow=true]

\lstdefinestyle{mystyle}{
	backgroundcolor=\color{backcolor},
	commentstyle=\color{codegray},
	keywordstyle=\color{blue},
	numberstyle=\tiny\color{codegray},
	stringstyle=\color{red},
	basicstyle=\ttfamily\footnotesize,
	breaklines=true,
	captionpos=b,
	keepspaces=true,
	numbersep=5pt,
	showspaces=false,
	showstringspaces=false,
	showtabs=false,
	tabsize=2,
	inputencoding=utf8,
	extendedchars=true,
}

\lstset{style=mystyle}

\title{Trabajo Práctico 1}
\author{Cisnero, Seivane, Serafini}
\date{22 de Septiembre de 2025}

\begin{document}


\begin{frame}
	\centering
	\includegraphics[width=0.25\textwidth]{UNAHUR (2)}
	\vfill
	{\huge \textbf{Trabajo Práctico 1}}\\[0.2cm]
	{\Large Aprendizaje Automático Avanzado}\\
	\vfill
	{\large Cisnero Matias, Seivane Nicolás, Serafini Franco}\\
	{\small 22 de Septiembre de 2025}
\end{frame}


\section{Ejercicio 1: Creación de Corpus}

\begin{frame}{}
	\centering
	\Large Ejercicio 1: Creación de Corpus\\
	\vspace{0.8cm}
	\includegraphics[width=0.5\textwidth]{imagen_cortazar}
\end{frame}
	
\begin{frame}[fragile]{1.1 Descripción de Librerías Usadas}
	
	\justifying
	Se utilizaron las librerías de $r$ y $pdfplumber$, en la cual utilizamos la ultima para leer página por página de un pdf y la primera para seleccionar las palabras.\\
	\vspace{0.1cm}
	Los links a las librerías son los siguientes.\\
	\vspace{0.1cm}
	\href{https://pypi.org/project/pdfplumber/#extracting-text}{\textbf{pdfplumber}}\\
	\vspace{0.1cm}
	\href{https://docs.python.org/es/3.13/library/re.html}{\textbf{r}}
	\vspace{0.2cm}
	
\begin{block}{Funciones Utilizadas}
	\begin{columns}[c]
		\column{0.45\textwidth}
		\begin{itemize}
			\item \texttt{pdfplumber.open() as pdf}
			\item \texttt{pdf.pages[]}
			\item \texttt{.extract\_text()}
			\item \texttt{.split('\textbackslash n')}
		\end{itemize}
		
		\column{0.45\textwidth}
		\begin{itemize}
			\item \texttt{re.findall()}
			\item \texttt{.endswith()}
			\item \texttt{.strip()}
			\item \texttt{.isdigit()}
			\item \texttt{.split('\textbackslash n')}
		\end{itemize}
	\end{columns}
\end{block}
	
\end{frame}

		%pdfplumber.open() = se utiliza para ir a la dirección del pdf, retornando una instancia de la% clase $pdfplumber.PDF$ 
%$pdf.pages[]$ = es una propiedad de la clase $pdfplumber.PDF$, la cual se puede indexar para acceder a %las paginas del pdf representadas en la clase $pdfplumber.Page$
%$.extract_text()$ = Método de la clase $pdfplumber.Page$, recopila todos los objetos de caracteres de %la página en un sol string.
%$split('\_n')$ = Divide el string que se genero antes en los saltos de pagina, generando una lista de %lineas.
%$re.findall()$ =
%$.endswith()$ =
%$.strip()$ =
%$.isdigit()$ =

\begin{frame}[fragile]{1.2.1 Estructura de Código}
	
	\justifying
	\textbf{\underline{Se utiliza la siguiente estructura de codigo:}}\\
	\vspace{0.1cm}
	Se comienza importando las librerías y creando una lista de palabras, donde se irán agregando las extracciones de texto.
	\begin{lstlisting}[language=Python]
import pdfplumber
import re

words = []
	\end{lstlisting}
	\justifying
	En lo cual se sigue utilizando la función \texttt{pdfplumber.open() as pdf}, en la cual se debe especificar la ruta hacia el pdf. El cual nos devuelve $pdf$ como una instancia de la clase \texttt{pdfplumber.PDF}
		\begin{lstlisting}[language=Python]
with pdfplumber.open("ruta") as pdf:
	\end{lstlisting}
\end{frame}

	
\begin{frame}[fragile]{1.2.2 Estructura de Código}
	
	\justifying
	Se continua utilizando una propiedad de la clase \texttt{pdfplumber.Page}, de la cual se puede indexar para acceder a las paginas del pdf
	\begin{lstlisting}[language=Python]
with pdfplumber.open("ruta") as pdf:
	for page in pdf.pages[:]:
	\end{lstlisting}
	\justifying
	En lo cual se utiliza el metodo \texttt{.extract\_text()}, que recopila todos los objetos de caracteres de la página en un solo string.
	\begin{lstlisting}[language=Python]
with pdfplumber.open("ruta") as pdf:
	for page in pdf.pages[:]:
		text = page.extract_text()
		if text:
	\end{lstlisting}
\end{frame}
	
\begin{frame}[fragile]{1.2.3 Estructura de Código}
	
	\justifying
	Se continua diviendo el string segun el metodo \texttt{.split('\textbackslash n')}, el cual devuleve una lista de strings, los cuales fueron separados de acuerdo a \textbackslash n, ergo saltos de linea.
	\begin{lstlisting}[language=Python]
with pdfplumber.open("ruta") as pdf:
	for page in pdf.pages[:]:
		text = page.extract_text()
			if text:
				lines = text.split('\n')
	\end{lstlisting}
	\justifying
	Luego se sacan las lineas que sean numeros de pagina tanto en el pie de la misma como en el encabezado. La forma de extraccion varia de acuerdo a como es el pdf.
	\begin{lstlisting}[language=Python]
				if lines[-1].strip().isdigit():
					lines = lines[:-1]
				if lines[0].strip().isdigit():
					lines = lines[1:]
				
	\end{lstlisting}
\end{frame}
	
\begin{frame}[fragile]{1.2.4 Estructura de Código}
	
	\justifying
	Se crea por linea una lista con el método de la librería r:\\
	\vspace{0.1cm}
	\texttt{re.findall(r"\_w+|[.,!?;:]", line)}, en el cual se separan con expresiones regulares las palabras con \textbackslash w+ y aparte los signos de puntuación con [.,!?;:], en una lista de strings. Luego para cada palabra se la pasa a minúscula con el método \texttt{.lower()}.
	\begin{lstlisting}[language=Python]
with pdfplumber.open("ruta") as pdf:
	for page in pdf.pages[:]:
		text = page.extract_text()
		if text:
			lines = text.split('\n')
			if lines[-1].strip().isdigit():
				lines = lines[:-1]
			if lines[0].strip().isdigit():
				lines = lines[1:]
			for line in lines:
				tokens = re.findall(r"\w+|[.,!?;:]", line)
				tokens = [token.lower() for token in tokens]
	\end{lstlisting}

\end{frame}

	
\begin{frame}[fragile]{1.2.5 Estructura de Código}
	
	\justifying
	Luego se diferencia por linea los puntos aparte, los cuales consideeramos los ultimos puntos de las lineas. Cada linea, las cuales fueron convertidas en listas de strings son agregadas a la lista del corpus\\

	\begin{lstlisting}[language=Python]
with pdfplumber.open("ruta") as pdf:
	for page in pdf.pages[:]:
		text = page.extract_text()
			if text:
				lines = text.split('\n')
				if lines[-1].strip().isdigit():
					lines = lines[:-1]
				if lines[0].strip().isdigit():
					lines = lines[1:]
				for line in lines:
					tokens = re.findall(r"\_w+|[.,!?;:]", line)
					tokens = [token.lower() for token in tokens]
				if line.endswith("."):
					tokens[-1]= ". "
				words.extend(tokens)
	\end{lstlisting}
	
	
%%% RAYUELA LOCOOOO	
	
	
	
\end{frame}
	
\begin{frame}{1.3 Libros utilizados: Rayuela}
	\justifying
	\textbf{Titulo:} Rayuela\\
	\textbf{Autor:} Julio Cortazar\\
	\textbf{Año :} 1963\\
	Se extrayeron 197.342 caracteres y 20.810 caracteres únicos que conforman el vocabulario.\\
	\centering
	\vspace{0.2cm}
	\includegraphics[width=0.3\textwidth]{rayuela_cortazar}
	
\end{frame}
	
\begin{frame}[fragile]{1.3.2 Código utilizado: Rayuela}
\begin{lstlisting}[language=Python]
with pdfplumber.open("Julio-Cortazar-Rayuela.pdf") as pdf:
	for page in pdf.pages[7:]:
		text = page.extract_text()
			if text:
				lines = text.split('\n')
				if lines[-1].strip().isdigit():
					lines = lines[:-1]
				if lines[0].strip().isdigit():
					lines = lines[1:]
				if lines[0].strip().isdigit():
					lines = lines[1:]
				if lines[-1].strip().isdigit():
					lines = lines[:-1]
				for line in lines:
					tokens = re.findall(r"\w+|[.,!?;:]", line)
					tokens = [token.lower() for token in tokens]
				if line.endswith("."):
					tokens[-1]= ". "
				words.extend(tokens)
\end{lstlisting}
	
\end{frame}
	
	
\begin{frame}{1.3.3 Ejemplo Borrado: Rayuela}
	\centering
	\includegraphics[width=0.5\textwidth]{borrado_rayuela}
	
\end{frame}	
	
	
%%% TODOS LOS FUEGOS

\begin{frame}{1.3 Libros utilizados: Todos los fuegos}
\justifying
\textbf{Titulo:} Todos los fuegos el fuego.\\
\textbf{Autor:} Julio Cortazar\\
\textbf{Año :} 1966\\
Se extrayeron 55.948 caracteres y 2.828 caracteres únicos que conforman el vocabulario.\\
\centering
\vspace{0.2cm}
\includegraphics[width=0.3\textwidth]{todos_los_fuegos_cortazar}

\end{frame}

\begin{frame}[fragile]{1.3.2 Código utilizado: Todos los fuegos}
\begin{lstlisting}[language=Python]
with pdfplumber.open("Julio Cortazar Todos los fuegos.pdf") as pdf:
	for page in pdf.pages[:-1]:
		text = page.extract_text()
			if text:
				lines = text.split('\n')
				if lines[-1].strip().isdigit():
					lines = lines[:-1]
				for line in lines:
					tokens = re.findall(r"\w+|[.,!?;:]", line)
					tokens = [token.lower() for token in tokens]
				if line.endswith("."):
					tokens[-1]= ". "
				words.extend(tokens)
		\end{lstlisting}

\end{frame}


\begin{frame}{1.3.3 Ejemplo Borrado: Todos los fuegos}
\centering
\includegraphics[width=0.5\textwidth]{borrado_todoslosfuegos}

\end{frame}	
	
%%% un tal lucas

\begin{frame}{1.3 Libros utilizados: Historias de cronopios y de famas}
	\justifying
	\textbf{Titulo:} Historias de cronopios y de famaso.\\
	\textbf{Autor:} Julio Cortazar\\
	\textbf{Año :} 1962\\
	Se extrayeron 32.224 caracteres y 2.514 caracteres únicos que conforman el vocabulario.\\
	\centering
	\vspace{0.2cm}
	\includegraphics[width=0.3\textwidth]{historias_de_cronopios_y_de_famas_cortazar}
	
\end{frame}

\begin{frame}[fragile]{1.3.2 Código utilizado: Historias de cronopios y de famas}
	\justifying
	En este caso no fue necesario quitar ninguna linea.
	\begin{lstlisting}[language=Python]
with pdfplumber.open("Historias-de-Cronopios-y-de-Famas - Julio Cortazar.pdf") as pdf:
	for page in pdf.pages[3:-1]:
		text = page.extract_text()
			if text:
				lines = text.split('\n')
				for line in lines:
					tokens = re.findall(r"\w+|[.,!?;:]", line)
					tokens = [token.lower() for token in tokens]
				if line.endswith("."):
					tokens[-1]= ". "
				words.extend(tokens)
	\end{lstlisting}
	
\end{frame}
	
	
%% ahora si un tal lucas

\begin{frame}{1.3 Libros utilizados: Un tal Lucas.}
	\justifying
	\textbf{Titulo:} Un tal Lucas.\\
	\textbf{Autor:} Julio Cortazar\\
	\textbf{Año :} 1979\\
	Se extrayeron 32.224 caracteres y 2.514 caracteres únicos que conforman el vocabulario.\\
	\centering
	\vspace{0.2cm}
	\includegraphics[width=0.3\textwidth]{un_tal_lucas_cortazar}
	
\end{frame}

\begin{frame}[fragile]{1.3.2 Código utilizado: Un tal Lucas.}
	\begin{lstlisting}[language=Python]

with pdfplumber.open("Lucas_Julio_Cortazar.pdf") as pdf:
	for page in pdf.pages[5:]:
		text = page.extract_text()
			if text:
				lines = text.split('\n')
				if lines[-1].strip().isdigit():
					lines = lines[:-1]
				lines = lines[1:]
				for line in lines:
					tokens = re.findall(r"\w+|[.,!?;:]", line)
					tokens = [token.lower() for token in tokens]
				if line.endswith("."):
					tokens[-1]= ". "
				words.extend(tokens)
	\end{lstlisting}
	
\end{frame}


\begin{frame}{1.3.3 Ejemplo Borrado: Un tal Lucas.}
	\centering
	\includegraphics[width=0.5\textwidth]{borrado_un_tal_lucas}
\end{frame}	
	
	
\begin{frame}[fragile]{1.4 Corpus final}
	\justifying
	Se guarda el corpus final en un archivo llamado \textbf{corpus.txt}.
	
	\begin{lstlisting}[language=Python]
		with open("corpus.txt", "w", encoding="utf-8") as f:
			f.write("\n".join(words))
	\end{lstlisting}
	
	Obteniendo un corpus como se ve en la siguiente imagen.
	\vspace{0.1cm}
\begin{columns}[t] % [t] para alinear arriba
	\column{0.55\textwidth}
	\textbf{Vocabulario (único):} 27.971\\
	\textbf{Corpus total:} 310.347
	
	\column{0.45\textwidth}
	\includegraphics[width=0.5\linewidth]{corpus_txt.png}
\end{columns}

\end{frame}


\section{Ejercicio 2: Implementación CBOW y SkipGram}

\begin{frame}{}
	\centering
	\Large Ejercicio 2: Implementación CBOW y SkipGram\\
	\vspace{0.8cm}
	\includegraphics[width=0.5\textwidth]{imagenes_cbow_skipgram}
\end{frame}



	
\begin{frame}[fragile]{2.1 Pasos previos}
	\justifying
	Se abre y carga el \textbf{corpus.txt}.
	
	\begin{lstlisting}[language=Python]
with open("corpus.txt", "r", encoding="utf-8") as f:
	corpus = f.read().splitlines()
	\end{lstlisting}
	Luego se crean los diccionarios que se utilizaran en ambos métodos.
	\begin{lstlisting}[language=Python]
    vocab = sorted(set(corpus))
		vocab_tamano = len(vocab)
		palabra_a_indice = {palabra: i for i, palabra in enumerate(vocab)}
		indice_a_palabra = {i: palabra for i, palabra in enumerate(vocab)}
	\end{lstlisting}

\end{frame}
	
	
\section{Ejercicio 2. A:CBOW}
	
\begin{frame}[fragile]{2.2 CBOW}
	\begin{block}{\textbf{Definición:} Continuous Bag of Words(CBOW)}
	\justifying
	\vspace{0.1cm}
	\textbf{Propósito:} Es un modelo de aprendizaje automático para aprender representaciones de palabras que capturan el "significado" de las palabras basadas en su contexto.\\
	\vspace{0.1cm}
	\textbf{Principio:} A diferencia de los modelos más simples, CBOW utiliza un contexto de $C$ palabras para predecir una palabra central\\
	\vspace{0.1cm}
	\textbf{Contexto vs. Predicción:}  A partir de un contexto de $C$ palabras ($p_{I,1}, p_{I,2}, ..., p_{I,C}$), se intenta predecir la palabra objetivo ($p_O$), que generalmente es la palabra central
\end{block}
	
\end{frame}
	
	
\begin{frame}[fragile]{2.2 CBOW: Conceptos Fundamentales}
	\begin{block}{\textbf{Definición:} Continuous Bag of Words(CBOW)}
		\justifying
		\vspace{0.1cm}
		\textbf{Propósito:} Es un modelo de aprendizaje automático para aprender representaciones de palabras que capturan el "significado" de las palabras basadas en su contexto.\\
		\vspace{0.1cm}
		\textbf{Principio:} A diferencia de los modelos más simples, CBOW utiliza un contexto de $C$ palabras para predecir una palabra central\\
		\vspace{0.1cm}
		\textbf{Contexto vs. Predicción:}  A partir de un contexto de $C$ palabras ($p_{I,1}, p_{I,2}, ..., p_{I,C}$), se intenta predecir la palabra objetivo ($p_O$), que generalmente es la palabra central
	\end{block}
	
\end{frame}

\begin{frame}[fragile]{2.2 CBOW: Arquitectura del Perceptrón}
	\begin{block}{\textbf{Arquitectura}}
		\justifying
		\vspace{0.1cm}
		\textbf{Estructura:} El calculo se enmarca en un \textbf{perceptron multicapa}\\
		\vspace{0.1cm}
		\textbf{Entrada:} Se presentan las $C$ palabras de contexto. La representación de las entradas se realiza mediante la \textbf{codificación One-hot}\\
		\vspace{0.1cm}
		\textbf{Diccionario ($V$):} Las palabras pertenecen a un diccionario $V$ cuyo cardinal es $|V|$ \\
		\vspace{0.1cm}
		\textbf{Capa Oculta:} Tiene una única capa oculta con $N$ unidades, todas con función de activación lineal $g(x) = x$ \\
		\vspace{0.1cm}
		\textbf{Dimensiones de Pesos:} 						\\
		\begin{itemize}
			\item Matriz de pesos Entrada-Oculta ($W$): $W \in |V| \times N$\\
			\item Matriz de pesos Oculta-Salida ($W'$): $W' \in N \times |V|$
		\end{itemize}
		\vspace{0.1cm}
		\textbf{Salida:} La función de activación de las unidades de salida es soft-max 
	\end{block}
	
\end{frame}
	
	
\begin{frame}[fragile]{2.2 CBOW: Arquitectura del Perceptrón}
\centering
\includegraphics[width=0.45\textwidth]{CBOW_arquitectura}
	
\end{frame}


\begin{frame}[fragile]{2.2 Calculo de representación Contextual (h)}

		\justifying
		\textbf{Cálculo de $h$:} Se $h$ calcula como el \textbf{promedio de las filas} de $W$ que corresponden a los índices de las palabras en el contexto\\
		\vspace{0.1cm}
		\textbf{Representaciones One-Hot:}  Sea $x_1, x_2, ..., x_C$ las representaciones one-hot de las $C$ palabras en el contexto\\
		\vspace{0.1cm}
		\textbf{Representaciones Contextuales ($v_p$):} Sea $v_{p_i}$ la representación contextual de la palabra $p_i$ (que es la fila $i$ de $W$ donde $i$ es el índice de $p_i$ en $V$) \\
		\vspace{0.1cm}
		\textbf{Fórmula de $h$:} 						\\
		\begin{itemize}
			\item Cálculo del vector $h$: 	
			$$h = \frac{1}{C} W^t(x_1 + x_2 + \dots + x_C)$$\\
			\item Cálculo de $h$ usando $v_p$:
			 $$h = \frac{1}{C} (v_{p1} + v_{p2} + \dots + v_{pC})$$
		\end{itemize}
	\begin{lstlisting}[language=Python]
		with open("corpus.txt", "r", encoding="utf-8") as f:
		corpus = f.read().splitlines()
	\end{lstlisting}
	Luego se crean los diccionarios que se utilizaran en ambos métodos.
	\begin{lstlisting}[language=Python]
		vocab = sorted(set(corpus))
		vocab_tamano = len(vocab)
		palabra_a_indice = {palabra: i for i, palabra in enumerate(vocab)}
		indice_a_palabra = {i: palabra for i, palabra in enumerate(vocab)}
	\end{lstlisting}
	
\end{frame}

	
\begin{frame}[fragile]{2.2 Calculo de representación Contextual (h)}

	\begin{lstlisting}[language=Python]
		h = cp.mean(W[indices_contextos], axis=0).reshape(-1,1)
	\end{lstlisting}
	Donde se utiliza la función \texttt{cupy.mean(a, axis=None, dtype=None, out=None, keepdims=False)}, que retorna la media de la matriz de entrada $a$ a lo largo del eje. 
	
	\begin{block}{Explicación}
	\begin{itemize}
	\item W[indices$\_$contextos] selecciona los vectores de embedding correspondientes a esas palabras\\
	\item axis=0 significa promediar columna por columna.
	\[
	h = \frac{1}{C} \sum_{i=1}^{C} v_{p_i}
	= \left(
	\frac{1}{C}\sum_{i=1}^C w_{i1}, \;
	\frac{1}{C}\sum_{i=1}^C w_{i2}, \;
	\dots, \;
	\frac{1}{C}\sum_{i=1}^C w_{id}
	\right)
	\]
\end{itemize}
	\end{block}
\end{frame}



\begin{frame}[fragile]{2.2 Salida de la Red}
	\begin{block}{Excitación de la salida}
\justifying
\textbf{ Vector de Salida ($v'_{p_j}$):} La columna $j$ de la matriz $W'$ se denota como $v'_{p_j}$, siendo el vector de salida para la palabra $p_j$\\
\vspace{0.3cm}
\textbf{Estado de Excitación ($u_j$)} El estado de excitación de cada unidad de salida $j$ ($u_j$) se calcula como el producto interno del vector de salida $v'_{p_j}$ y la activación de la capa oculta $h$\\
$$u_j = (\mathbf{v}'_{p_j})^t \mathbf{h} \quad$$
Siendo en el código
\begin{lstlisting}[language=Python]
	u = W_prima.T@h
\end{lstlisting}
	\end{block}
\end{frame}
	
	
\begin{frame}[fragile]{2.2 Salida de la Red (Softmax)}
	\begin{block}{Excitación de la salida}
		\justifying
		\textbf{ Activación de Salida ($y_j$):} La función de activación de las unidades de salida es soft-max. El estado de activación $y_j$ de la unidad $j$ se calcula de la siguiente manera\\
		$$y_j = \frac{\exp u_j}{\sum_{j'=1}^{|V|} \exp u_{j'}} \quad$$
		\vspace{0.3cm}
		\textbf{ Interpretación:} El valor de $y_j$ se interpreta como la probabilidad de que la palabra objetivo sea $p_j$ dado el contexto que entra a la red ($p_{I,1}, \dots, p_{I,C}$)\\
		$$P(p_j/p_{I,1}, \dots, p_{I,C}) = y_j \quad$$
		Siendo en el código
		\begin{lstlisting}[language=Python]
			y = softmax(u)
		\end{lstlisting}
		Donde 
				\begin{lstlisting}[language=Python]
		def softmax(u):
			u_max = np.max(u)
			e_u = np.exp(u - u_max)
			return e_u / e_u.sum()
		\end{lstlisting}
	\end{block}
\end{frame}
	
	
\begin{frame}[fragile]{2.2 Salida de la Red (Softmax)}
	\begin{block}{Excitación de la salida}
		\justifying
		Siendo en el código
		\begin{lstlisting}[language=Python]
			def softmax(u):
			u_max = np.max(u)
			e_u = np.exp(u - u_max)
			return e_u / e_u.sum()
		\end{lstlisting}
		\textbf{ Consideraciones:} Se le resta a $u$ el maximo de $u$, ya que generaba errores $nan$ en los pesos\\
		\vspace{0.2cm}
		\textbf{ Explicación:} Restar $u\_max$ evita problemas numéricos por exponentes grandes, llamado overflow\\
		\[
		\frac{e^{u_i - c}}{\sum_j e^{u_j - c}} 
		= \frac{e^{u_i} e^{-c}}{\sum_j e^{u_j} e^{-c}} 
		= \frac{e^{u_i}}{\sum_j e^{u_j}}
		\]

	\end{block}
\end{frame}
	
	
\begin{frame}[fragile]{2.2 Función de Pérdida ($E$)}
	\begin{block}{}
		\justifying
		\textbf{Objetivo:} Minimizar la función de pérdida $E$, que es el negativo del logaritmo de la probabilidad de la palabra objetivo $p_O$ dado el contexto ($p_{I,1}, \dots, p_{I,C}$).\\[0.2cm]
		
		\textbf{Pérdida Logarítmica:} 
		\[
		E = - \log p(p_O \mid p_{I,1}, p_{I,2}, \dots, p_{I,C})
		\] \\[0.1cm]
		
		\textbf{Error ($e_j$):} Se define como la diferencia entre la predicción y el objetivo:
		\[
		e_j = y_j - t_j
		\]
		donde
		\[
		t_j =
		\begin{cases}
			1 & \text{si } j = j^* \text{ (índice de la palabra objetivo $p_O$)}\\
			0 & \text{si } j \neq j^*
		\end{cases}
		\] \\[0.1cm]
	\end{block}

\end{frame}
	
\begin{frame}[fragile]{2.2 Función de Pérdida y Derivada}
	\begin{block}{Función de Pérdida}
		La función de pérdida se define como:
\[
\begin{aligned}
	E &= - \log p(p_O \mid p_{I,1}, p_{I,2}, \dots, p_{I,C}) \\
	&= - u_{j^*} + \log \sum_{j'=1}^{|V|} \exp u_{j'} \\
\end{aligned}
\]
		\textbf{Derivada del error respecto al estado de excitación $u_j$:}
		\[
		\frac{\partial E}{\partial u_j} = y_j - t_j = e_j
		\]
			En código es
		\begin{lstlisting}[language=Python]
			e = y
			e[indice_central] -= 1
		\end{lstlisting}

	\end{block}
\end{frame}
	
\begin{frame}[fragile]{2.2 Actualización de $W'$ (Pesos Oculta-Salida)}
	\begin{block}{Actualización de pesos}
	La regla de actualización para $W'$ en CBOW es idéntica a la del contexto de una sola palabra, ya que solo se modificó el cálculo de $h$\\
	Derivando $E$ respecto a $w'_{ij}$ obtenemos la regla de actualización:\\
	En forma vectorial:
	\[
	W'_{\cdot,j}(\text{nuevo}) = W'_{\cdot,j}(\text{anterior}) - \eta \, e_j \, h
	\]
	Considerando las columnas de $W'$ como vectores de salida $v'_j$:
	\[
	v'_j(\text{nuevo}) = v'_j(\text{anterior}) - \eta \, e_j \, h
	\]
	En código es
	\begin{lstlisting}[language=Python]
		W_prima -= n*(h@e.T)
	\end{lstlisting}
	
	
	
	\end{block}
\end{frame}



\begin{frame}[fragile]{2.2 Vector de Error Propagado ($EH$):}
	\begin{block}{Error Propagado}
		Este vector de error se propaga hacia la capa oculta y se calcula como el producto de la matriz $W'$ por el vector de errores de salida $e$\\
		\[
		EH = W' \mathbf{e}
		\]
		La regla de actualización para $W$ se aplica a la fila $W_lc$ de $W$ (o su vector de representación contextual $v_{l_c}$) para cada palabra $l_c$ en el contexto, donde $1 \leq c \leq C$
		\[
		\mathbf{v}{l_c}(\text{nuevo}) = \mathbf{v}{l_c}(\text{anterior}) - \eta \frac{1}{C} EH^t
		\]
		En código es
		\begin{lstlisting}[language=Python]
			EH = W_prima@e
			W[indices_contextos] -=n * EH.T / len(indices_contextos)
		\end{lstlisting}
	\end{block}
\end{frame}

	
\begin{frame}[fragile]{2.2 Código Entero}
	\begin{block}{Código Python}
		\begin{lstlisting}[language=Python]
for epoca in range(epocas):	
		for i, (indice_central, indices_contextos) in enumerate(indices_tuplas):
			h = cp.mean(W[indices_contextos], axis=0).reshape(-1,1)
			u = W_prima.T @ h
			y = softmax(u)
			
			e = y
			e[indice_central] -= 1
			W_prima -= n * (h @ e.T)
			EH = W_prima @ e
			W[indices_contextos] -= n * EH.T / len(indices_contextos)
		\end{lstlisting}
	\end{block}
\end{frame}
	
	
	
	
	
	
\end{document}