% presentacion_cbob_frag_tablas.tex
\documentclass[11pt]{beamer}
\usetheme{CambridgeUS}


\usepackage[spanish]{babel}
\usepackage[utf8]{inputenc}
\usepackage{booktabs}
\usepackage{caption}

\title[CBOW + Fragmentación]{Cómo la fragmentación del corpus mejora CBOW: \\ evaluación por palabra con accuracies top-1 y top-5}
\author[N. Seivane]{Nicolás Seivane}
\institute[Tu Institución]{Departamento / Laboratorio \\ Tu Institución}
\date{\today}

\begin{document}
	
	\begin{frame}
		\titlepage
	\end{frame}
	
	\begin{frame}{Contenido}
		\tableofcontents
	\end{frame}
	
	\section{Motivación}
	\begin{frame}{Motivación}
		\begin{itemize}
			\item CBOW aprende representaciones de palabras a partir de contextos locales.
			\item En corpora extensos y heterogéneos, una misma palabra puede tener significados o usos diferentes según el dominio.
			\item \textbf{Hipótesis:} fragmentar el corpus en subconjuntos coherentes mejora la capacidad del modelo para representar con precisión las palabras, aumentando el accuracy top-1 y top-5.
		\end{itemize}
	\end{frame}
	
	\section{CBOW y fragmentación}
	\begin{frame}{Estrategia}
		\begin{itemize}
			\item Corpus dividido en $m$ fragmentos temáticos o por dominio.
			\item Se entrena un CBOW por fragmento y luego se combinan las representaciones de cada palabra mediante:
			\begin{enumerate}
				\item Promedio de embeddings.
				\item Concatenación de embeddings.
				\item Fine-tuning global posterior.
			\end{enumerate}
			\item Evaluación: precisión top-1 y top-5 sobre un conjunto de predicciones por palabra.
		\end{itemize}
	\end{frame}
	
	\section{Resultados comparativos}
	\begin{frame}{Accuracy global vs fragmentado}
		\begin{table}
			\centering
			\begin{tabular}{lcc}
				\toprule
				\textbf{Modelo} & \textbf{Accuracy@1} & \textbf{Accuracy@5} \\
				\midrule
				CBOW global & 0.741 & 0.892 \\
				CBOW fragmentado (promedio) & 0.782 & 0.917 \\
				CBOW fragmentado (concatenado) & \textbf{0.806} & \textbf{0.931} \\
				CBOW fragmentado (fine-tuned) & 0.798 & 0.925 \\
				\bottomrule
			\end{tabular}
			\caption{Comparación general de accuracies (valores de ejemplo).}
		\end{table}
	\end{frame}
	
	\begin{frame}{Accuracy por palabra representativa}
		\begin{table}
			\centering
			\begin{tabular}{lcccc}
				\toprule
				\textbf{Palabra} & \textbf{CBOW global} & \textbf{Promedio} & \textbf{Concat.} & \textbf{Fine-tuned} \\
				\midrule
				energía & 0.69 / 0.87 & 0.74 / 0.91 & \textbf{0.78 / 0.93} & 0.76 / 0.92 \\
				salud & 0.73 / 0.88 & 0.79 / 0.92 & \textbf{0.82 / 0.94} & 0.80 / 0.93 \\
				economía & 0.76 / 0.90 & 0.80 / 0.92 & \textbf{0.83 / 0.95} & 0.81 / 0.94 \\
				ambiente & 0.71 / 0.86 & 0.77 / 0.90 & \textbf{0.80 / 0.93} & 0.79 / 0.91 \\
				educación & 0.68 / 0.84 & 0.74 / 0.89 & \textbf{0.78 / 0.92} & 0.76 / 0.91 \\
				\bottomrule
			\end{tabular}
			\caption{Accuracy top-1 / top-5 por palabra (valores ilustrativos).}
		\end{table}
	\end{frame}
	
	\section{Discusión}
	\begin{frame}{Discusión}
		\begin{itemize}
			\item Los modelos fragmentados logran sistemáticamente mejores accuracies.
			\item El ensamblado por concatenación ofrece la mejor precisión, aunque a costa de mayor dimensionalidad.
			\item El promedio de embeddings es una opción eficiente con mejoras moderadas.
			\item Las palabras polisémicas se benefician más de la fragmentación, ya que cada fragmento modela un contexto más homogéneo.
		\end{itemize}
	\end{frame}
	
	\section{Conclusiones}
	\begin{frame}{Conclusiones}
		\begin{itemize}
			\item La fragmentación del corpus mejora los resultados de CBOW, reflejado en mayores accuracies top-1 y top-5.
			\item Los vocabularios compuestos por fragmento permiten representar de forma más contextual las palabras.
			\item Trabajo futuro: extender a modelos contextuales (Skip-gram, BERT fragmentado) y explorar combinación de embeddings vía reducción (PCA o autoencoders).
		\end{itemize}
	\end{frame}
	
\end{document}